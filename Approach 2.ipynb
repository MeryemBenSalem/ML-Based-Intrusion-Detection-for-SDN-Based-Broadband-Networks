{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d78eecf-2e75-4ac7-8a9e-1709ee7d4cf1",
   "metadata": {},
   "source": [
    "# Network Intrusion Detection Pipeline\n",
    "\n",
    "This pipeline outlines the steps involved in creating a machine learning model for detecting network intrusions using the NSL-KDD dataset. The process involves data preprocessing, handling class imbalance, feature engineering, model training, and evaluation using various classifiers.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Project Setup\n",
    "\n",
    "### 1.1 Import Libraries\n",
    "Start by importing the necessary Python libraries such as pandas, numpy, sklearn, and imbalanced-learn for data manipulation, preprocessing, and model evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Load and Prepare the Data\n",
    "\n",
    "### 2.1 Load the Dataset\n",
    "We use two datasets: a training set and a test set. The NSL-KDD dataset is loaded into pandas DataFrames from the provided URLs.\n",
    "\n",
    "### 2.2 Initial Data Inspection\n",
    "Inspect the dimensions of both the training and test sets to understand the size of the data and ensure the data has been loaded correctly.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Data Preprocessing\n",
    "\n",
    "### 3.1 Data Cleaning\n",
    "Perform data cleaning by:\n",
    "- Removing spaces and converting column names to lowercase for consistency.\n",
    "- Dropping rows with missing values to ensure clean data.\n",
    "- Removing duplicate rows to avoid redundant information.\n",
    "\n",
    "### 3.2 Data Type Optimization\n",
    "Convert high-memory data types (e.g., int64, float64) to more memory-efficient types (e.g., int32, float32) to reduce memory usage during processing.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Feature Engineering\n",
    "\n",
    "### 4.1 Separating Categorical and Numerical Features\n",
    "Identify and separate the categorical columns (e.g., protocol_type, service, flag) from the numerical features. This step is necessary for different treatment in the preprocessing pipeline.\n",
    "\n",
    "### 4.2 Feature Scaling and Encoding\n",
    "- Scale numerical features to standardize them.\n",
    "- Apply one-hot encoding to categorical features to convert them into numerical form that machine learning algorithms can understand.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Handling Class Imbalance\n",
    "\n",
    "### 5.1 Oversampling with Random Oversampler\n",
    "Since the dataset may have an imbalanced distribution of classes (e.g., more normal connections than attack connections), we use Random Oversampling to balance the classes. This technique replicates samples from the minority class to ensure equal representation during model training.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Feature Extraction\n",
    "\n",
    "### 6.1 Clustering with K-Means\n",
    "Introduce clustering as a way to generate meta-features:\n",
    "- Apply K-Means clustering to the oversampled data to identify underlying patterns or groups.\n",
    "- Add the resulting cluster labels as a new feature to the dataset to enhance the predictive power of the model.\n",
    "\n",
    "### 6.2 Dimensionality Reduction with PCA\n",
    "Apply Principal Component Analysis (PCA) to reduce the dimensionality of the dataset while preserving as much variance as possible. This helps in reducing the complexity of the model while retaining key information.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Model Selection and Training\n",
    "\n",
    "### 7.1 Model Selection\n",
    "Select various machine learning models to train and evaluate:\n",
    "- **Decision Tree**\n",
    "- **Random Forest**\n",
    "- **Extra Trees**\n",
    "\n",
    "\n",
    "These models are chosen for their ability to handle classification problems effectively, especially with structured tabular data.\n",
    "\n",
    "### 7.2 Training with K-Fold Cross-Validation\n",
    "Perform 10-fold cross-validation on the training dataset to:\n",
    "- Split the dataset into 10 equal parts.\n",
    "- Train the model on 9 parts and validate it on the 10th part.\n",
    "- Repeat this process 10 times with different splits to ensure the model generalizes well to unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Model Evaluation\n",
    "\n",
    "### 8.1 Evaluation Metrics\n",
    "Evaluate each model using the following metrics:\n",
    "- **Accuracy**: The proportion of correct predictions to the total predictions.\n",
    "- **Precision**: The proportion of true positive predictions to all positive predictions.\n",
    "- **Recall**: The proportion of true positive predictions to all actual positives.\n",
    "- **F1-Score**: The harmonic mean of precision and recall.\n",
    "- **ROC Curve**: Visualize the trade-off between true positive rate and false positive rate.\n",
    "\n",
    "### 8.2 Compare Model Performance\n",
    "Compare the performance of all models based on the evaluation metrics. Identify the model with the best performance for further tuning and deployment.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Final Model Selection\n",
    "\n",
    "After evaluating the models, select the one with the best performance (e.g., based on accuracy, F1-score, or other metrics). This model can be used for further testing on the test dataset or future deployment.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Conclusion\n",
    "\n",
    "\n",
    "\n",
    "The following table summarizes the performance of the three machine learning models evaluated in the pipeline:\n",
    "\n",
    "| **Model**       | **Accuracy Scores (per fold)**                                                                                                                                                               | **Average Accuracy** |\n",
    "|-----------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------|\n",
    "| Decision Tree   | [0.9997, 0.9996, 0.9997, 0.9997, 0.9996, 0.9997, 0.9997, 0.9997, 0.9997, 0.9997]                                                                                                             | 0.9997               |\n",
    "| Random Forest   | [0.9998, 0.9998, 0.9998, 0.9998, 0.9997, 0.9998, 0.9998, 0.9998, 0.9998, 0.9998]                                                                                                             | 0.9998               |\n",
    "| Extra Trees     | [0.9998, 0.9998, 0.9998, 0.9998, 0.9997, 0.9998, 0.9999, 0.9998, 0.9999, 0.9998]                                                                                                             | 0.9998               |\n",
    "\n",
    "From the evaluation, both **Random Forest** and **Extra Trees** models performed slightly better than the **Decision Tree**, with an average accuracy of **0.9998** for both. The **Decision Tree** also achieved strong performance with an average accuracy of **0.9997**. Therefore, both Random Forest and Extra Trees are recommended for further tuning and potential deployment.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b22f1fd3-3a22-4d23-8ed0-6973fa7eee1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the Training set: (125973, 42)\n",
      "Dimensions of the Test set: (22544, 42)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import sklearn\n",
    "import io\n",
    "import random\n",
    "train_url = 'https://raw.githubusercontent.com/merteroglu/NSL-KDD-Network-Instrusion-Detection/master/NSL_KDD_Train.csv'\n",
    "test_url = 'https://raw.githubusercontent.com/merteroglu/NSL-KDD-Network-Instrusion-Detection/master/NSL_KDD_Test.csv'\n",
    "col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\"]\n",
    "\n",
    "\n",
    "df = pd.read_csv(train_url,header=None, names = col_names)\n",
    "\n",
    "df_test = pd.read_csv(test_url, header=None, names = col_names)\n",
    "\n",
    "print('Dimensions of the Training set:',df.shape)\n",
    "print('Dimensions of the Test set:',df_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bfb1124-479b-419b-b1c6-6dbb65fa54c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Decision Tree...\n",
      "Decision Tree - Accuracy scores for each fold: [0.9996965568891271, 0.9995868008703007, 0.9997223818347333, 0.9996642757071192, 0.9995738883974976, 0.9997030131255286, 0.9996771881799223, 0.9997159255983317, 0.9997352943075364, 0.999696554930014]\n",
      "Decision Tree - Average accuracy: 0.9997\n",
      "Evaluating Random Forest...\n",
      "Random Forest - Accuracy scores for each fold: [0.9997675754895441, 0.9997869441987488, 0.9998256816171581, 0.9998063129079534, 0.9997030131255286, 0.9998192253807565, 0.9997934004351503, 0.9998256816171581, 0.9998321378535596, 0.9997804865451164]\n",
      "Random Forest - Average accuracy: 0.9998\n",
      "Evaluating Extra Trees...\n",
      "Extra Trees - Accuracy scores for each fold: [0.9997934004351503, 0.9997740317259457, 0.9998192253807565, 0.9997998566715519, 0.9997352943075364, 0.9998385940899612, 0.9998515065627643, 0.9998450503263627, 0.9998515065627643, 0.9997804865451164]\n",
      "Extra Trees - Average accuracy: 0.9998\n",
      "\n",
      "Final Model Evaluation:\n",
      "Decision Tree - Mean accuracy: 0.9997\n",
      "Random Forest - Mean accuracy: 0.9998\n",
      "Extra Trees - Mean accuracy: 0.9998\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "#  Preprocessing for df and df_test\n",
    "def preprocess_df(df):\n",
    "    # Replace column names with no spaces and lowercase\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "\n",
    "    # Handle missing values (drop rows with missing values for simplicity)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Drop duplicate rows\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # Convert int64 to int32 and float64 to float32 to reduce memory usage\n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        df[col] = df[col].astype(np.int32)\n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = preprocess_df(df)\n",
    "df_test = preprocess_df(df_test)\n",
    "\n",
    "# Feature Scaling for both df and df_test\n",
    "# Separate categorical and numerical columns\n",
    "categorical_cols = ['protocol_type', 'service', 'flag']\n",
    "numeric_cols = df.select_dtypes(include=['int32', 'float32']).columns.tolist()\n",
    "\n",
    "# Define transformers for categorical and numerical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', OneHotEncoder(), categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Prepare features and labels for df\n",
    "X_train = df.drop(columns=['label'])\n",
    "y_train = df['label']\n",
    "\n",
    "# Prepare features for df_test (labels are not included for test data)\n",
    "X_test = df_test.drop(columns=['label'])\n",
    "y_test = df_test['label'] if 'label' in df_test.columns else None\n",
    "\n",
    "# Encode output features (labels) for df\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.fit_transform(y_test)\n",
    "\n",
    "# Fit and transform the input features for df and df_test\n",
    "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "#  Handle Class Imbalance Using Random Oversampling (RO)\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "# Apply Random Oversampling to the training data\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train_preprocessed, y_train_encoded)\n",
    "X_test_resampled, y_test_resampled = ros.fit_resample(X_test_preprocessed, y_test_encoded)\n",
    "\n",
    "#  Clustering for Meta-Features\n",
    "n_clusters = 10\n",
    "\n",
    "# Apply KMeans clustering to the resampled dataset\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X_train_resampled)\n",
    "\n",
    "# Add cluster labels as a new feature to the dataset\n",
    "X_train_resampled_with_clusters = np.hstack((X_train_resampled, cluster_labels.reshape(-1, 1)))\n",
    "\n",
    "#  Feature Extraction using PCA\n",
    "n_components = 20 \n",
    "\n",
    "# Apply PCA to the resampled dataset with clusters\n",
    "pca = PCA(n_components=n_components, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train_resampled_with_clusters)\n",
    "\n",
    "# Apply the same steps to the test data\n",
    "# Apply KMeans clustering to the test data (using the already fitted model)\n",
    "cluster_labels_test = kmeans.predict(X_test_resampled)\n",
    "\n",
    "# Add cluster labels as a new feature to the test dataset\n",
    "X_test_resampled_with_clusters = np.hstack((X_test_resampled, cluster_labels_test.reshape(-1, 1)))\n",
    "\n",
    "# Apply PCA to the test data using the already fitted PCA model\n",
    "X_test_pca = pca.transform(X_test_resampled_with_clusters)\n",
    "\n",
    "# Define the number of splits (k-fold)\n",
    "k = 10\n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize the models\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Extra Trees': ExtraTreesClassifier(random_state=42),\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "accuracy_scores = {model_name: [] for model_name in models}\n",
    "\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "\n",
    "   \n",
    "    fold_accuracy_scores = []\n",
    "\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_train_pca):  # X_train_pca is  PCA-transformed features\n",
    "        # Split the data into train and test sets using the indices provided by kf.split\n",
    "        X_train, X_test = X_train_pca[train_index], X_train_pca[test_index]\n",
    "        y_train, y_test = y_train_resampled[train_index], y_train_resampled[test_index]\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Evaluate the model \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        \n",
    "        fold_accuracy_scores.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy for this model across all folds\n",
    "    average_accuracy = np.mean(fold_accuracy_scores)\n",
    "\n",
    "    \n",
    "    accuracy_scores[model_name] = fold_accuracy_scores\n",
    "\n",
    "   \n",
    "    print(f\"{model_name} - Accuracy scores for each fold: {fold_accuracy_scores}\")\n",
    "    print(f\"{model_name} - Average accuracy: {average_accuracy:.4f}\")\n",
    "\n",
    "# Print the final accuracy scores for all models\n",
    "print(\"\\nFinal Model Evaluation:\")\n",
    "for model_name, scores in accuracy_scores.items():\n",
    "    print(f\"{model_name} - Mean accuracy: {np.mean(scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46e7f575-61a9-46bc-bb95-8761e2525d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('ML-Based IDS.pickle','wb') as f:\n",
    "    pickle.dump(model,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
